{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOWdyEX6IwyU"
      },
      "outputs": [],
      "source": [
        "!pip install syntheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoqBxNiwKC8i"
      },
      "outputs": [],
      "source": [
        "from syntheval import SynthEval\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-35BeEsKXt6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6jFrnOMl0kY"
      },
      "outputs": [],
      "source": [
        "data_sizes = ['1000', '2500', '5000', '10000', '20000', '50000']\n",
        "data_regions = ['national', 'ca', 'tx']\n",
        "synthesizers = ['CTGAN', 'LLM', 'synthpop', 'TVAE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuHQRxYH6EDx"
      },
      "outputs": [],
      "source": [
        "# Function to load, round, drop NA, reset index, and remove homogenous columns\n",
        "def load_and_process_data(file_path):\n",
        "    df = pd.read_csv(file_path).dropna().reset_index(drop=True).round(0)\n",
        "    if 'X' in df.columns:\n",
        "        df = df.drop(columns=['X'])\n",
        "    if 'row_index' in df.columns:\n",
        "        df = df.drop(columns=['row_index'])\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.drop(columns=['Unnamed: 0'])\n",
        "    homogenous_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "    df = df.drop(columns=homogenous_cols)\n",
        "    return df\n",
        "\n",
        "# Load and process testing and training data\n",
        "def load_all_data(data_sizes, data_regions, load_func, data_type):\n",
        "    dataframes = {region: {} for region in data_regions}\n",
        "    for region in data_regions:\n",
        "        for size in data_sizes:\n",
        "            file_path = f'/content/drive/My Drive/06_kdd/02_data/{data_type}/{data_type}_{region}_{size}.csv'\n",
        "            dataframes[region][size] = load_func(file_path)\n",
        "    return dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LTh5K_uXBhi"
      },
      "outputs": [],
      "source": [
        "testing_dataframes = load_all_data(data_sizes, data_regions, load_and_process_data, \"testing\")\n",
        "training_dataframes = load_all_data(data_sizes, data_regions, load_and_process_data, \"training\")\n",
        "\n",
        "synthesized_dataframes = {synth: {region: {size: {} for size in data_sizes} for region in data_regions} for synth in synthesizers}\n",
        "\n",
        "for synth in synthesizers:\n",
        "    for region in data_regions:\n",
        "        for size in data_sizes:\n",
        "            for run in range(5):\n",
        "                file_path = f'/content/drive/My Drive/06_kdd/02_data/{synth}/training_{region}_{size}_Run_{run}.csv'\n",
        "                synthesized_dataframes[synth][region][size][run] = load_and_process_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjCW8Jn5itS7"
      },
      "outputs": [],
      "source": [
        "categorical_columns_national = ['state', 'Parties_Description', 'EthnicGroups_EthnicGroup1Desc', 'Ethnic_Description','Residence_HHParties_Description', 'CommercialData_PropertyType', 'voted', 'Voters_Gender', 'nonpartisan_donation']\n",
        "categorical_columns_no_state = ['Parties_Description', 'EthnicGroups_EthnicGroup1Desc', 'Ethnic_Description','Residence_HHParties_Description', 'CommercialData_PropertyType', 'voted', 'Voters_Gender', 'nonpartisan_donation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbH2phypKHG0"
      },
      "outputs": [],
      "source": [
        "categorical_columns_national_synthpop = ['state', 'Parties_Description', 'EthnicGroups_EthnicGroup1Desc','Residence_HHParties_Description', 'CommercialData_PropertyType', 'voted', 'Voters_Gender', 'nonpartisan_donation']\n",
        "categorical_columns_no_state_synthpop = ['Parties_Description', 'EthnicGroups_EthnicGroup1Desc','Residence_HHParties_Description', 'CommercialData_PropertyType', 'voted', 'Voters_Gender', 'nonpartisan_donation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67LvlVAZkmHe"
      },
      "outputs": [],
      "source": [
        "voted = 'voted'\n",
        "donation = 'nonpartisan_donation'\n",
        "party = 'Residence_HHParties_Description'\n",
        "\n",
        "metrics_all = {\n",
        "        \"corr_diff\" : {\"mixed_corr\": True},\n",
        "        \"mi_diff\"   : {},\n",
        "        \"ks_test\"   : {\"sig_lvl\": 0.05, \"n_perms\": 1000},\n",
        "        \"p_mse\"     : {\"k_folds\": 5, \"max_iter\": 1000, \"solver\": \"liblinear\"},\n",
        "        \"cls_acc\"   : {\"F1_type\": \"micro\", \"k_folds\": 5},\n",
        "        \"dcr\"       : {},\n",
        "        \"eps_risk\"  : {},\n",
        "        \"mia_risk\"  : {\"num_eval_iter\": 5},\n",
        "        \"att_discl\" : {}\n",
        "    }\n",
        "\n",
        "metrics_gen_util = {\n",
        "    \"corr_diff\" : {\"mixed_corr\": True},\n",
        "    \"mi_diff\"   : {},\n",
        "    \"ks_test\"   : {\"sig_lvl\": 0.05, \"n_perms\": 1000},\n",
        "    \"p_mse\"     : {\"k_folds\": 5, \"max_iter\": 1000, \"solver\": \"liblinear\"},\n",
        "}\n",
        "\n",
        "metrics_target = {\n",
        "        \"cls_acc\"   : {\"F1_type\": \"micro\", \"k_folds\": 3}\n",
        "    }\n",
        "\n",
        "metrics_privacy = {\n",
        "    # \"dcr\"       : {},\n",
        "    \"eps_risk\"  : {},\n",
        "    # \"mia_risk\"  : {\"num_eval_iter\": 5},\n",
        "    # \"att_discl\" : {}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Cg6jk15eOR"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and save results for synthesized data\n",
        "def evaluate_synthesized_data(real_df, test_df, synth_df, cat_cols, metrics, base_filename, size, run, metric_type, target='empty'):\n",
        "    print(f\"Evaluating {base_filename}_{size}_Run_{run}...\")\n",
        "\n",
        "    # Capture the output\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = mystdout = io.StringIO()\n",
        "\n",
        "    # Run the evaluation\n",
        "    if target == 'empty':\n",
        "      S = SynthEval(real_df, holdout_dataframe=test_df, cat_cols=cat_cols)\n",
        "      _ = S.evaluate(synth_df, **metrics)\n",
        "    else:\n",
        "      S = SynthEval(real_df, holdout_dataframe=test_df, cat_cols=cat_cols)\n",
        "      _ = S.evaluate(synth_df, target, **metrics)\n",
        "\n",
        "    # Reset stdout\n",
        "    sys.stdout = old_stdout\n",
        "\n",
        "    # Get the captured output\n",
        "    evaluation_results = mystdout.getvalue()\n",
        "\n",
        "    # Write evaluation results to a text file\n",
        "    if target == 'empty':\n",
        "      text_file_name = f'{base_filename}_{size}_Run_{run}_{metric_type}_evaluation_results.txt'\n",
        "    else:\n",
        "      text_file_name = f'{base_filename}_{size}_Run_{run}_{metric_type}_{target}_evaluation_results.txt'\n",
        "    with open(text_file_name, 'w') as f:\n",
        "        f.write(evaluation_results)\n",
        "\n",
        "    # # Save and download the text file\n",
        "    # files.download(text_file_name)\n",
        "    # time.sleep(2)  # Add a short delay to ensure the download completes\n",
        "\n",
        "    # Save and downlaod text file to google drive\n",
        "    drive_file_name = text_file_name\n",
        "    drive_file_path = f'/content/drive/My Drive/06_kdd/'\n",
        "    drive_file_path = drive_file_path + drive_file_name\n",
        "    with open(drive_file_path, 'w') as f:\n",
        "        f.write(evaluation_results)\n",
        "\n",
        "    # files.download(drive_file_path)\n",
        "    # time.sleep(2)  # Add a short delay to ensure the download completes\n",
        "\n",
        "    sys.stdout.flush()\n",
        "    sys.stderr.flush()\n",
        "\n",
        "    # Save all the figures generated by syntheval\n",
        "    for j, figure in enumerate(plt.get_fignums()):\n",
        "        plt.figure(figure)\n",
        "        plot_file_name = f'{base_filename}_{size}_Run_{run}_figure_{j + 1}.png'\n",
        "        plt.savefig(plot_file_name)\n",
        "\n",
        "        drive_file_path = f'/content/drive/My Drive/06_kdd/'\n",
        "        drive_file_path = drive_file_path + plot_file_name\n",
        "        with open(drive_file_path, 'w') as f:\n",
        "            f.write(evaluation_results)\n",
        "\n",
        "        # files.download(drive_file_path)\n",
        "        # time.sleep(2)  # Add a short delay to ensure the download completes\n",
        "\n",
        "        # files.download(plot_file_name)\n",
        "        # time.sleep(2)  # Add a short delay to ensure the download completes\n",
        "        sys.stdout.flush()\n",
        "        sys.stderr.flush()\n",
        "\n",
        "    # Close all figures to avoid overlapping in the next iteration\n",
        "    plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbFKDSNqlMOR"
      },
      "source": [
        "## CTGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxkvK6bL1173"
      },
      "source": [
        "### General - CTGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECv6H02Q4_x3"
      },
      "outputs": [],
      "source": [
        "# CTGAN National Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'ctgan_national'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivMBoA1deCgd"
      },
      "outputs": [],
      "source": [
        "# CTGAN ca Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'ctgan_ca'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLIz9DMcj9Gj"
      },
      "outputs": [],
      "source": [
        "# CTGAN tx Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'ctgan_tx'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKFMCp3e17ZZ"
      },
      "source": [
        "### Target - CTGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJMCnyWlS9dw"
      },
      "outputs": [],
      "source": [
        "# CTGAN National Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'ctgan_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgHFy4cClN-Z"
      },
      "outputs": [],
      "source": [
        "# CTGAN National Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'ctgan_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEulM3A0lOmw"
      },
      "outputs": [],
      "source": [
        "# CTGAN National Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'ctgan_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwzwKSmsWHDZ"
      },
      "outputs": [],
      "source": [
        "# CTGAN ca Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'ctgan_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTq20pbyWV-_"
      },
      "outputs": [],
      "source": [
        "# CTGAN ca Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'ctgan_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPyNN9dnWW0H"
      },
      "outputs": [],
      "source": [
        "# CTGAN ca Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'ctgan_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEZhbH172GGZ"
      },
      "outputs": [],
      "source": [
        "# CTGAN tx Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'ctgan_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ_zwSrD2IDV"
      },
      "outputs": [],
      "source": [
        "# CTGAN tx Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'ctgan_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD64usmq2MZQ"
      },
      "outputs": [],
      "source": [
        "# CTGAN tx Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'ctgan_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['CTGAN']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdQ9eZP3LlV"
      },
      "source": [
        "## synthpop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHVryHb53LlW"
      },
      "source": [
        "### General - synthpop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSFHUn9z3LlX"
      },
      "outputs": [],
      "source": [
        "# synthpop National Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'synthpop_national'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['synthpop']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national_synthpop, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YFTIprx3LlX"
      },
      "outputs": [],
      "source": [
        "# synthpop ca Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'synthpop_ca'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['synthpop']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWSDwCgo3LlY"
      },
      "outputs": [],
      "source": [
        "# synthpop tx Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'synthpop_tx'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['synthpop']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ-cbMFa3LlZ"
      },
      "source": [
        "### Target - synthpop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElXF675x3LlZ"
      },
      "outputs": [],
      "source": [
        "# synthpop National Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'synthpop_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['synthpop']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national_synthpop, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_-3sJcz3LlZ"
      },
      "outputs": [],
      "source": [
        "# synthpop National Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'synthpop_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['synthpop']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national_synthpop, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aU81FhI3Lla"
      },
      "outputs": [],
      "source": [
        "# synthpop National Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'synthpop_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['national'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national_synthpop, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZUKDIuH3Lla"
      },
      "outputs": [],
      "source": [
        "# synthpop ca Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'synthpop_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['ca'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRA1eDgn3Lla"
      },
      "outputs": [],
      "source": [
        "# synthpop ca Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'synthpop_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['ca'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xob7mFPp3Llb"
      },
      "outputs": [],
      "source": [
        "# synthpop ca Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'synthpop_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['ca'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0sKgRQ23Llb"
      },
      "outputs": [],
      "source": [
        "# synthpop tx Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'synthpop_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['tx'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGZ-e3eD3Llb"
      },
      "outputs": [],
      "source": [
        "# synthpop tx Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'synthpop_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['tx'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NClogfV3Llc"
      },
      "outputs": [],
      "source": [
        "# synthpop tx Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'synthpop_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "\n",
        "        real_df = real_df.drop(columns=['Ethnic_Description'])\n",
        "        test_df = test_df.drop(columns=['Ethnic_Description'])\n",
        "\n",
        "        for run in range(5):\n",
        "            try:\n",
        "               synth_df = synthesized_dataframes['synthpop']['tx'][size][run]\n",
        "               evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state_synthpop, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f2C9wdIZNHe"
      },
      "source": [
        "## LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frD7QZZHZRRO"
      },
      "source": [
        "### General - LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aimvc-BDj-ef"
      },
      "outputs": [],
      "source": [
        "# LLM National Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'LLM_national'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AESFhrJtj-ef"
      },
      "outputs": [],
      "source": [
        "# LLM ca Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'LLM_ca'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8h62rpAj-ef"
      },
      "outputs": [],
      "source": [
        "# LLM tx Level GENERAL UTILITY Evaluation\n",
        "base_filename = 'LLM_tx'\n",
        "metric_type = 'general_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['tx'][size][run]\n",
        "        evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_gen_util, base_filename, size, run, metric_type)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJSal7gzZTs0"
      },
      "source": [
        "### Target - LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUjQVx9HkxaN"
      },
      "outputs": [],
      "source": [
        "# LLM National Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'LLM_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9ijtelykxaO"
      },
      "outputs": [],
      "source": [
        "# LLM National Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'LLM_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-10MDzIkxaO"
      },
      "outputs": [],
      "source": [
        "# LLM National Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'LLM_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP3OdPb4kxaO"
      },
      "outputs": [],
      "source": [
        "# LLM ca Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'LLM_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZbmhNZ4kxaO"
      },
      "outputs": [],
      "source": [
        "# LLM ca Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'LLM_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLdoAZlFkxaP"
      },
      "outputs": [],
      "source": [
        "# LLM ca Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'LLM_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CzvBtA7kxaP"
      },
      "outputs": [],
      "source": [
        "# LLM tx Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'LLM_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01siHt23kxaP"
      },
      "outputs": [],
      "source": [
        "# LLM tx Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'LLM_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epcGOC_HkxaP"
      },
      "outputs": [],
      "source": [
        "# LLM tx Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'LLM_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['LLM']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQZh6zuv8E3k"
      },
      "source": [
        "### Target - SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7nxGtld8E3w"
      },
      "outputs": [],
      "source": [
        "# SMOTE National Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'SMOTE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5PNy33n8E3x"
      },
      "outputs": [],
      "source": [
        "# SMOTE National Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'SMOTE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsmdTCeK8E3x"
      },
      "outputs": [],
      "source": [
        "# SMOTE National Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'SMOTE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzxIgjtH8E3x"
      },
      "outputs": [],
      "source": [
        "# SMOTE ca Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'SMOTE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrfC7h_y8E3x"
      },
      "outputs": [],
      "source": [
        "# SMOTE ca Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'SMOTE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arNLf_k48E3y"
      },
      "outputs": [],
      "source": [
        "# SMOTE ca Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'SMOTE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FanNIQd38E3y"
      },
      "outputs": [],
      "source": [
        "# SMOTE tx Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'SMOTE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4MuPLcph8E3y"
      },
      "outputs": [],
      "source": [
        "# SMOTE tx Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'SMOTE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNC2ArOn8E3z"
      },
      "outputs": [],
      "source": [
        "# SMOTE tx Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'SMOTE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['SMOTE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99YtwKAxmC1e"
      },
      "source": [
        "### Target - TVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g27Mb6rHmC1h"
      },
      "outputs": [],
      "source": [
        "# TVAE National Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'TVAE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target=voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rouIaOkPmC1i"
      },
      "outputs": [],
      "source": [
        "# TVAE National Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'TVAE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Q6pCTVmC1i"
      },
      "outputs": [],
      "source": [
        "# TVAE National Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'TVAE_national'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['national'][size]\n",
        "        test_df = testing_dataframes['national'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['national'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_national, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cDmcq9l8mC1i",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# TVAE ca Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'TVAE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "maDAwTximC1i"
      },
      "outputs": [],
      "source": [
        "# TVAE ca Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'TVAE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJpj3hdgmC1i"
      },
      "outputs": [],
      "source": [
        "# TVAE ca Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'TVAE_ca'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['ca'][size]\n",
        "        test_df = testing_dataframes['ca'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['ca'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSVZomB5mC1j"
      },
      "outputs": [],
      "source": [
        "# TVAE tx Level TARGET-SPECIFIC UTILITY Evaluation - VOTED\n",
        "base_filename = 'TVAE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = voted)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3V5aaNhKmC1j"
      },
      "outputs": [],
      "source": [
        "# TVAE tx Level TARGET-SPECIFIC UTILITY Evaluation - DONATION\n",
        "base_filename = 'TVAE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = donation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7NN0s6MmC1j"
      },
      "outputs": [],
      "source": [
        "# TVAE tx Level TARGET-SPECIFIC UTILITY Evaluation - PARTY\n",
        "base_filename = 'TVAE_tx'\n",
        "metric_type = 'target_util'\n",
        "\n",
        "\n",
        "for size in data_sizes:\n",
        "    try:\n",
        "        real_df = training_dataframes['tx'][size]\n",
        "        test_df = testing_dataframes['tx'][size]\n",
        "        for run in range(5):\n",
        "            try:\n",
        "                synth_df = synthesized_dataframes['TVAE']['tx'][size][run]\n",
        "                evaluate_synthesized_data(real_df, test_df, synth_df, categorical_columns_no_state, metrics_target, base_filename, size, run, metric_type, target = party)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for size {size}, run {run}: {e}\")\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing data for size {size}: {e}\")\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uHVryHb53LlW",
        "jZ-cbMFa3LlZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}